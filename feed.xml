<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="http://fras.uk/feed.xml" rel="self" type="application/atom+xml" /><link href="http://fras.uk/" rel="alternate" type="text/html" /><updated>2021-03-18T08:05:32-05:00</updated><id>http://fras.uk/feed.xml</id><title type="html">Fraser Greenlee</title><subtitle>ML projects et al.</subtitle><entry><title type="html">Interpolating the internet</title><link href="http://fras.uk/ml/2021/03/11/Interpolating-the-internet.html" rel="alternate" type="text/html" title="Interpolating the internet" /><published>2021-03-11T00:00:00-06:00</published><updated>2021-03-11T00:00:00-06:00</updated><id>http://fras.uk/ml/2021/03/11/Interpolating%20the%20internet</id><author><name></name></author><category term="ML" /><summary type="html">Large scale Deep Learning models can combine abstract concepts in new ways. GPT-3 can write stories while DALLÂ·E makes images. These models are improving fast so I want to explore what improved versions will do and how they will change the internet.</summary></entry><entry><title type="html">An Improved Transformer-VAE</title><link href="http://fras.uk/ml/large%20prior-free%20models/transformer-vae/2021/02/23/An-Improved-Transformer-VAE.html" rel="alternate" type="text/html" title="An Improved Transformer-VAE" /><published>2021-02-23T00:00:00-06:00</published><updated>2021-02-23T00:00:00-06:00</updated><id>http://fras.uk/ml/large%20prior-free%20models/transformer-vae/2021/02/23/An%20Improved%20Transformer-VAE</id><author><name></name></author><category term="ML" /><category term="Large prior-free models" /><category term="Transformer-VAE" /><summary type="html"></summary></entry><entry><title type="html">Transformer-VAE for Program Synthesis</title><link href="http://fras.uk/ml/large%20prior-free%20models/code/transformer-vae/2020/08/25/Transformer-VAE-for-Program-Synthesis.html" rel="alternate" type="text/html" title="Transformer-VAE for Program Synthesis" /><published>2020-08-25T00:00:00-05:00</published><updated>2020-08-25T00:00:00-05:00</updated><id>http://fras.uk/ml/large%20prior-free%20models/code/transformer-vae/2020/08/25/Transformer-VAE%20for%20Program%20Synthesis</id><author><name></name></author><category term="ML" /><category term="Large prior-free models" /><category term="code" /><category term="Transformer-VAE" /><summary type="html">Motivation</summary></entry><entry><title type="html">Transformers as Variational Autoencoders</title><link href="http://fras.uk/ml/large%20prior-free%20models/transformer-vae/2020/08/13/Transformers-as-Variational-Autoencoders.html" rel="alternate" type="text/html" title="Transformers as Variational Autoencoders" /><published>2020-08-13T00:00:00-05:00</published><updated>2020-08-13T00:00:00-05:00</updated><id>http://fras.uk/ml/large%20prior-free%20models/transformer-vae/2020/08/13/Transformers%20as%20Variational%20Autoencoders</id><author><name></name></author><category term="ML" /><category term="Large prior-free models" /><category term="Transformer-VAE" /><summary type="html">Image generators based on Variational Autoencoders have had huge success. Unfortunately the same cannot be said for sequence generation. There seems to be little interest in this space but with Transformers it is now possible to learn smooth latent spaces of large structured sequences.</summary></entry><entry><title type="html">An Avatar game with realistic physics</title><link href="http://fras.uk/games/simulation/2020/07/08/An-Avatar-game-with-realistic-physics.html" rel="alternate" type="text/html" title="An Avatar game with realistic physics" /><published>2020-07-08T00:00:00-05:00</published><updated>2020-07-08T00:00:00-05:00</updated><id>http://fras.uk/games/simulation/2020/07/08/An%20Avatar%20game%20with%20realistic%20physics</id><author><name></name></author><category term="games" /><category term="simulation" /><summary type="html">After watching Avatar: The Last Airbender I wanted to experience bending the elements just like in the show. Of course actually doing this is impossible but could a game give that feeling?</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="https://raw.githubusercontent.com/Fraser-Greenlee/fastpages/master/images/katara_waterbend_arrows.gif" /><media:content medium="image" url="https://raw.githubusercontent.com/Fraser-Greenlee/fastpages/master/images/katara_waterbend_arrows.gif" xmlns:media="http://search.yahoo.com/mrss/" /></entry></feed>