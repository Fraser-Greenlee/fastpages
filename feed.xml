<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.1.1">Jekyll</generator><link href="http://fras.uk/feed.xml" rel="self" type="application/atom+xml" /><link href="http://fras.uk/" rel="alternate" type="text/html" /><updated>2021-08-17T02:38:23-05:00</updated><id>http://fras.uk/feed.xml</id><title type="html">Fraser Greenlee</title><subtitle>ML projects et al.</subtitle><entry><title type="html">Making a Transformer-VAE with JAX.</title><link href="http://fras.uk/ml/2021/06/13/Making-a-Transformer-VAE-with-JAX.html" rel="alternate" type="text/html" title="Making a Transformer-VAE with JAX." /><published>2021-06-13T00:00:00-05:00</published><updated>2021-06-13T00:00:00-05:00</updated><id>http://fras.uk/ml/2021/06/13/Making%20a%20Transformer-VAE%20with%20JAX</id><author><name></name></author><category term="ML" /><summary type="html">JAX allows writing simple code that runs efficiently on TPUs. These models can then operate on massive scales setting new benchmarks in performance.</summary></entry><entry><title type="html">Interpolating the internet</title><link href="http://fras.uk/ml/2021/03/11/Interpolating-the-internet.html" rel="alternate" type="text/html" title="Interpolating the internet" /><published>2021-03-11T00:00:00-06:00</published><updated>2021-03-11T00:00:00-06:00</updated><id>http://fras.uk/ml/2021/03/11/Interpolating%20the%20internet</id><author><name></name></author><category term="ML" /><summary type="html">Large scale Deep Learning models can combine abstract concepts in new ways. GPT-3 can write stories while DALLÂ·E makes images. These models are improving fast so I want to explore what improved versions will do and how they will change the internet.</summary></entry><entry><title type="html">An Improved Transformer-VAE</title><link href="http://fras.uk/ml/large%20prior-free%20models/transformer-vae/2021/02/23/An-Improved-Transformer-VAE.html" rel="alternate" type="text/html" title="An Improved Transformer-VAE" /><published>2021-02-23T00:00:00-06:00</published><updated>2021-02-23T00:00:00-06:00</updated><id>http://fras.uk/ml/large%20prior-free%20models/transformer-vae/2021/02/23/An%20Improved%20Transformer-VAE</id><author><name></name></author><category term="ML" /><category term="Large prior-free models" /><category term="Transformer-VAE" /><summary type="html"></summary></entry><entry><title type="html">Transformer-VAE for Program Synthesis</title><link href="http://fras.uk/ml/large%20prior-free%20models/code/transformer-vae/2020/08/25/Transformer-VAE-for-Program-Synthesis.html" rel="alternate" type="text/html" title="Transformer-VAE for Program Synthesis" /><published>2020-08-25T00:00:00-05:00</published><updated>2020-08-25T00:00:00-05:00</updated><id>http://fras.uk/ml/large%20prior-free%20models/code/transformer-vae/2020/08/25/Transformer-VAE%20for%20Program%20Synthesis</id><author><name></name></author><category term="ML" /><category term="Large prior-free models" /><category term="code" /><category term="Transformer-VAE" /><summary type="html">Motivation</summary></entry><entry><title type="html">Transformers as Variational Autoencoders</title><link href="http://fras.uk/ml/large%20prior-free%20models/transformer-vae/2020/08/13/Transformers-as-Variational-Autoencoders.html" rel="alternate" type="text/html" title="Transformers as Variational Autoencoders" /><published>2020-08-13T00:00:00-05:00</published><updated>2020-08-13T00:00:00-05:00</updated><id>http://fras.uk/ml/large%20prior-free%20models/transformer-vae/2020/08/13/Transformers%20as%20Variational%20Autoencoders</id><author><name></name></author><category term="ML" /><category term="Large prior-free models" /><category term="Transformer-VAE" /><summary type="html">Image generators based on Variational Autoencoders have had huge success. Unfortunately the same cannot be said for sequence generation. There seems to be little interest in this space but with Transformers it is now possible to learn smooth latent spaces of large structured sequences.</summary></entry></feed>