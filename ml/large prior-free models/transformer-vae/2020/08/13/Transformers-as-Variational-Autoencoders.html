<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.6.1 -->
<title>Transformers as Variational Autoencoders | Fraser Greenlee</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Transformers as Variational Autoencoders" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Avoid posterior collapse even with a huge model." />
<meta property="og:description" content="Avoid posterior collapse even with a huge model." />
<link rel="canonical" href="https://fraser-greenlee.github.io/fastpages/ml/large%20prior-free%20models/transformer-vae/2020/08/13/Transformers-as-Variational-Autoencoders.html" />
<meta property="og:url" content="https://fraser-greenlee.github.io/fastpages/ml/large%20prior-free%20models/transformer-vae/2020/08/13/Transformers-as-Variational-Autoencoders.html" />
<meta property="og:site_name" content="Fraser Greenlee" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2020-08-13T00:00:00-05:00" />
<script type="application/ld+json">
{"url":"https://fraser-greenlee.github.io/fastpages/ml/large%20prior-free%20models/transformer-vae/2020/08/13/Transformers-as-Variational-Autoencoders.html","@type":"BlogPosting","headline":"Transformers as Variational Autoencoders","dateModified":"2020-08-13T00:00:00-05:00","datePublished":"2020-08-13T00:00:00-05:00","mainEntityOfPage":{"@type":"WebPage","@id":"https://fraser-greenlee.github.io/fastpages/ml/large%20prior-free%20models/transformer-vae/2020/08/13/Transformers-as-Variational-Autoencoders.html"},"description":"Avoid posterior collapse even with a huge model.","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/fastpages/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://fraser-greenlee.github.io/fastpages/feed.xml" title="Fraser Greenlee" /><!-- the google_analytics_id gets auto inserted from the config file -->



<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-JGPYQFPWNN"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-JGPYQFPWNN');
</script>


<link rel="shortcut icon" type="image/x-icon" href="/fastpages/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />


    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/fastpages/">Fraser Greenlee</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/fastpages/about/">About Me</a><a class="page-link" href="/fastpages/search/">Search</a><a class="page-link" href="/fastpages/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Transformers as Variational Autoencoders</h1><p class="page-description">Avoid posterior collapse even with a huge model.</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2020-08-13T00:00:00-05:00" itemprop="datePublished">
        Aug 13, 2020
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      5 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/fastpages/categories/#ML">ML</a>
        &nbsp;
      
        <a class="category-tags-link" href="/fastpages/categories/#Large prior-free models">Large prior-free models</a>
        &nbsp;
      
        <a class="category-tags-link" href="/fastpages/categories/#Transformer-VAE">Transformer-VAE</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <p>Image generators based on Variational Autoencoders have had huge success.
Unfortunately the same cannot be said for sequence generation.
There seems to be little interest in this space but with Transformers it is now possible to learn smooth latent spaces of large structured sequences.</p>

<p>To show this I’m releasing <a href="https://github.com/Fraser-Greenlee/T5-VAE">T5-VAE</a> a mod of Google’s <a href="https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html">Text-to-Text Transformer</a> to learn smooth latent spaces of sequences.</p>

<p><a href="https://colab.research.google.com/drive/126LRvBzTt4c5jqwn2PMq6NQ1sqiVsI_2?usp=sharing">Try using it in Google Colab.</a></p>

<p><a href="https://app.wandb.ai/fraser/T5-VAE?workspace=user-fraser">See Weights &amp; Biasis training runs.</a></p>

<p><a href="https://github.com/Fraser-Greenlee/T5-VAE">Check out the source code on GitHub.</a></p>

<ol id="markdown-toc">
  <li><a href="#from-autoencoders-to-mmd-vae" id="markdown-toc-from-autoencoders-to-mmd-vae">From Autoencoders to MMD-VAE</a></li>
  <li><a href="#a-transformer-as-an-mmd-vae" id="markdown-toc-a-transformer-as-an-mmd-vae">A Transformer as an MMD-VAE</a></li>
  <li><a href="#use-cases" id="markdown-toc-use-cases">Use cases</a></li>
</ol>

<h2 id="from-autoencoders-to-mmd-vae">From Autoencoders to MMD-VAE</h2>

<p>To understand how this works and the ways it differs from previous systems, it is important to know how an autoencoder works, specifically a Maximum Mean Discrepancy Variational Autoencoder.</p>

<p><a href="/fastpages/images/prog_synth/autoencoder.png"><img src="/fastpages/images/prog_synth/autoencoder.png" alt="." /></a></p>

<p>An Autoencoder learns to recreate its training data by compressing the data into a compressed representation called a “latent code” and decompressing it back into the original data.
Each latent code is just a vector of numbers with each number constrained within some bounds (between -1, 1 in this case by a <a href="https://pytorch.org/docs/stable/generated/torch.nn.Tanh.html#torch.nn.Tanh">Tanh function</a>).
You can think of each latent vector as a position on a latent map of input data.
In each direction on the map our input data changes in semantic ways.</p>

<p>The problem with an Autoencoder is that our latent map has holes.
These holes are where latent codes have no set meaning and so result in garbage output from the decoder.
To resolve this we use a Variational Autoencoder (VAE).
It has a regularising loss function which encourages a smooth distribution of latent codes.
This regularising loss encourages our latent codes to match a target probability distribution, usually a bell curve.
Now intermediate points on our map are also valid points meaning we can traverse it smoothly.</p>

<p>Unfortunately, when applying this model to sequences it doesn’t work.
The issue comes from our extra loss function and how we decode sequences.</p>

<p>Our regularising loss encourages the latent space to be smoother but if the loss is brought to near zero the space becomes meaningless.
This is tempered by the reconstruction loss which encourages the latent space to be informative.
If the decoder is generating a sequence, it has access to the previous tokens in its current sequence which during training are always correct.
When the decoder has the option of a slightly random latent code or a nonrandom previous output it ignores the latent code &amp; just looks at its previous tokens.</p>

<p>This is known as “posterior collapse” since the posterior is the probability of an event (the next token) given relevant information (the latent code).
The resulting model ignores the latent code and so just models a prior distribution of tokens.</p>

<p>To solve this the Maximum Mean Discrepancy Variational Autoencoder was made.
It is similar to a VAE but instead of the reconstruction loss, it uses an MMD (mean-maximum-discrepancy) loss.
The MMD loss measures the similarity between latent codes, between samples from the target distribution and between both latent codes &amp; samples.
It optimises the similarity between latent codes &amp; target samples separately to match the similarity between mixed samples.</p>

<p>This loss is much softer on the latest codes and solved posterior collapse for my use case.
If your keen to learn more about MMD-VAE you should check out <a href="https://ermongroup.github.io/blog/a-tutorial-on-mmd-variational-autoencoders/">this post</a>.</p>

<h2 id="a-transformer-as-an-mmd-vae">A Transformer as an MMD-VAE</h2>

<p>Lets put this model to use to generate some structured sequences.
The <a href="https://huggingface.co/transformers/model_doc/t5.html">T5 model</a> provided by Huggingface will create the Encoder &amp; Decoder for the sequences. To get a compressed encoding of the inputs, the inputs are first padded to ensure each the sequence is 12 tokens long. Finally, some fully-connected layers compress and then decompress the fixed length encodings. I’ve named this model “T5-VAE”.</p>

<p><a href="/fastpages/images/prog_synth/t5-vae.png"><img src="/fastpages/images/prog_synth/t5-vae.png" alt="." /></a></p>

<p>This model is then trained to recreate its input tokens with the MMD loss on its latent code. Once training is complete it is possible to start generating some sequences!</p>

<p>I tried out recreating single lines of code from my dataset of <a href="https://www.kaggle.com/frasergreenlee/python-state-changes">9 million Python state changes</a>.
This code comes from real coding solutions so the model will learn more useful snippets of code than if the data was random.
However, this also means the code snippets could be more varied.</p>

<p>Here I step through the latent space between 2 sample code snippets.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Intermediate Samples</span>
0.0 x <span class="o">=</span> a - 1<span class="p">;</span> <span class="c"># Starting latent space</span>
0.1 x <span class="o">=</span> a - 1<span class="p">;</span>
0.2 x <span class="o">=</span> a - 1<span class="p">;</span>
0.3 x <span class="o">=</span> a - 1<span class="p">;</span>
0.4 x <span class="o">=</span> a + 1<span class="p">;</span>
0.5 x <span class="o">=</span> a + 2<span class="p">;</span>
0.6 x <span class="o">=</span> a + 2<span class="p">;</span>
0.7 x <span class="o">=</span> a + 2 <span class="k">*</span> 2<span class="p">;</span>
0.8 x <span class="o">=</span> a + 10 <span class="k">*</span> 2<span class="p">;</span>
0.9 x <span class="o">=</span> a + 10 <span class="k">*</span> 2<span class="p">;</span>
1.0 x <span class="o">=</span> a + 10 <span class="k">*</span> 2<span class="p">;</span> <span class="c"># Ending latent space</span>
</code></pre></div></div>

<p>Here I randomly generate latent codes to see how common syntax errors are.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Randomly Sampled Sequences</span>
er <span class="o">=</span> int<span class="o">(</span>h[3] <span class="k">*</span> 0<span class="o">)</span><span class="p">;</span>
l.append<span class="o">([</span>False[j] <span class="k">*</span> d<span class="o">)</span><span class="p">;</span> <span class="c"># Invalid Code</span>
y <span class="o">=</span> <span class="s1">'[0 '</span><span class="o">]</span> <span class="o">=</span> 1<span class="p">;</span> <span class="c"># Invalid Code</span>
x <span class="o">=</span> int<span class="o">(</span>h[-1] <span class="k">*</span> 0<span class="o">)</span><span class="p">;</span>
l.append<span class="o">(</span> <span class="o">=</span> 0 + str<span class="o">(</span>x[0 / 1]<span class="o">)</span><span class="p">;</span> <span class="c"># Invalid Code</span>
x.append<span class="o">(</span>a[da] <span class="k">*</span> 0<span class="o">)</span><span class="p">;</span>
x <span class="o">=</span><span class="s1">''</span><span class="o">[</span>0 - 1:0]<span class="p">;</span>
x.append<span class="o">(</span>x.pop<span class="o">(</span>  + 1<span class="o">)</span> <span class="k">**</span> 0<span class="o">)</span><span class="p">;</span>
f <span class="o">=</span> int<span class="o">(</span>h[i].pop<span class="o">()</span> + 1<span class="o">)</span><span class="p">;</span>
x <span class="o">=</span> int<span class="o">(</span>h[-1 - 1]<span class="o">)</span><span class="p">;</span>
</code></pre></div></div>

<p>Though the intermediate values are good, just randomly sampling from the latent space occasionally produces invalid outputs.</p>

<h2 id="use-cases">Use cases</h2>

<p>Now that we can learn smooth latent spaces of sequences a lot is possible:</p>

<ul>
  <li>
    <p><strong>Learn a position in latent space</strong></p>

    <ul>
      <li>Train another model take T5-VAE encodings (e.g. representing a tweet) and predict some property (e.g. the number of likes).
Now you can get a loss based on your target number of likes and backpropagate that loss to change the latent position of a given tweet.
The result should be a tweet optimizer! I’ve got a demo of this coming soon.</li>
    </ul>
  </li>
  <li>
    <p><strong>Discover semantic changes in latent space</strong></p>

    <ul>
      <li>Change a sequence in one way (e.g. change the tone) and find the difference in latent space.
You may be able to apply that change in latent space to other sequences to get a tone changer.</li>
    </ul>
  </li>
</ul>

  </div><!-- from https://github.com/utterance/utterances -->
<script src="https://utteranc.es/client.js"
        repo="Fraser-Greenlee/fastpages"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script><a class="u-url" href="/fastpages/ml/large%20prior-free%20models/transformer-vae/2020/08/13/Transformers-as-Variational-Autoencoders.html" hidden></a>
</article>
      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/fastpages/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/fastpages/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/fastpages/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>ML projects et al.</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/Fraser-Greenlee" title="Fraser-Greenlee"><svg class="svg-icon grey"><use xlink:href="/fastpages/assets/minima-social-icons.svg#github"></use></svg></a></li><li><a rel="me" href="https://twitter.com/FraserGreenlee" title="FraserGreenlee"><svg class="svg-icon grey"><use xlink:href="/fastpages/assets/minima-social-icons.svg#twitter"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
